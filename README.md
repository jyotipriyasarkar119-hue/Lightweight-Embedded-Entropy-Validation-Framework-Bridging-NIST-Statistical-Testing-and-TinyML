<h1>Lightweight-Embedded-Entropy-Validation-Framework-Bridging-NIST-Statistical-Testing-and-TinyML</h1>

<h2>Introduction</h2>
<p>
The rapid expansion of embedded and Internet-of-Things (IoT) devices has significantly increased the demand for reliable, low-cost, and secure hardware-based random number generation. True Random Number Generators (TRNGs), which extract entropy from physical phenomena such as thermal noise, metastability, and SRAM startup states, are widely used in cryptographic systems to generate secure keys, nonces, and authentication tokens. However, ensuring the quality and unpredictability of such entropy sources remains a critical challenge, particularly in resource-constrained embedded environments.

To address this issue, standardized statistical validation frameworks have been developed by the National Institute of Standards and Technology (NIST). Documents such as NIST SP 800-22 and SP 800-90B define comprehensive methodologies for evaluating the statistical randomness and entropy content of bitstreams generated by hardware and software sources. While these test suites provide strong validation guarantees, they are computationally intensive, require large sample sizes, and are not suitable for real-time execution on low-memory microcontrollers. As a result, embedded systems typically rely on offline validation or simplified heuristics, leaving a gap between standardized entropy evaluation and practical deployment constraints.

Recent advances in Tiny Machine Learning (TinyML) offer a promising alternative. Lightweight neural networks, especially when quantized to low-bit precision (e.g., INT8 or INT4), can operate within strict memory and computational limits while performing complex pattern recognition tasks. This opens the possibility of approximating statistical randomness testing using compact neural models trained on features derived from NIST-based evaluations. Such an approach can potentially enable real-time entropy quality monitoring directly on embedded hardware.

This project proposes a Lightweight Embedded Entropy Validation Framework that bridges standardized NIST statistical testing and TinyML-based classification under extreme memory constraints. The framework integrates hardware entropy generation (e.g., analog noise and SRAM startup behavior), preprocessing techniques such as XOR folding and bit mixing, formal NIST statistical validation to establish ground-truth entropy labels, and a quantized neural network deployed on a resource-limited microcontroller. By systematically evaluating memory usage, inference latency, and classification accuracy across different quantization levels, this work investigates whether a compact embedded model can reliably approximate the results of computationally heavy statistical test suites.

The primary objective of this research is to determine the minimum computational and memory requirements necessary for accurate entropy validation in embedded systems. By combining hardware-level experimentation, standardized statistical testing, and TinyML deployment, the proposed framework aims to provide a scalable and practical solution for real-time entropy monitoring in next-generation secure embedded platforms.
</p>

<h2>Work Flow</h2>

Hardware Entropy Source<br>
        ↓<br>
Preprocessing (XOR / Folding / Mixing)<br>
        ↓<br>
Large Bitstream Collection<br>
        ↓<br>
NIST Statistical Testing (Ground Truth)<br>
        ↓<br>
Feature Extraction<br>
        ↓<br>
Tiny Quantized Neural Network<br>
        ↓<br>
Real-Time Entropy Classification (Embedded)<br>
